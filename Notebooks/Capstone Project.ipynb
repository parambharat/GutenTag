{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "## Machine Learning Engineer Nanodegree\n",
    "\n",
    "Bharat Ramanathan\n",
    "September 20, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "Today, computers perform a myriad of tasks and yet we are unable to communicate with them naturally. We humans use words, sound and gestures to communicate our ideas, thoughts and feelings. Yet computers only understand explicit instructions, that require complex artificial programming languages. Natural Language Processing (NLP) is a field of Machine Learning and AI that is concerned with human-computer interaction using Natural Language.  NLP aims to break the language barrier and allows us to interact with computers in a natural manner. Advances in NLP research is one of the primary reasons for the development of personal assistants, voice commands, search etc.\n",
    "\n",
    "In this project we are concerned with the ability of systems to process and understand written text. Natural Language processing is often difficult due to the lack of a strict structure in human languages and a variety of anomalies present in them. All this messiness results in challenges to process text in a meaningful way. A rule based system is not viable since the rules of language are plenty, often unrestricted and change over time.\n",
    "\n",
    "With text documents it often becomes difficult to find and discover what we are looking for. Traditionally this is done using search keywords and links. Imagine  instead the ability to search and explore documents based on the themes that run through them. We might first find the theme that we are interested in, and then examine the documents related to that theme. This project is about discovering thematic similarities in a collection of documents using Machine Learning and Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Searching fan-fiction documents is an onerous task. This is further emphasized when search tags are quite ineffective in finding relevant documents and seldom consider context and themes while retrieving documents. While document search is augmented by SEO (Search Engine Optimization) keywords they can be misleading often on purpose to improve page visits and user clicks. Other tags include genre tags which although quite accurate prove to be too general for the task of search. Users often struggle with finding the necessary documents using only genres.\n",
    "\n",
    "We seek to address the problem of document search by forming meaningful thematic categories that can be used to find relevant documents. Furthermore, these thematically similar documents can be utilized to build recommender systems. We intend to explore *probabilistic topic models* that represent documents as being generated from a group of topics. One way to think about the process of topic modelling is to assume that each document in a collection contains a mixture of topics. A topic can be thought of as a distribution of words that appear in similar syntactic and semantic context. Of course, all we have to begin with are the documents but the model specifies a simple probabilistic procedure by which documents can be generated from a distribution of topics. Standard statistical techniques are then used to invert this process, inferring the set of topics that were responsible for generating a collection of documents. We will further discuss topic models and how they work in the Algorithms and Techniques section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Project Imports\n",
    "%matplotlib inline\n",
    "import os\n",
    "from collections import defaultdict, OrderedDict\n",
    "from operator import itemgetter\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from settings import project_root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics<a id=\"metrics\"></a>\n",
    "\n",
    "The topic model's performance is evaluated by measuring its ability to improve the performance of a simple [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn-linear-model-sgdclassifier) to classify the documents into genres. We calculate the weighted `F1-Score` between predicted genre and the actual genre of documents on a test set.\n",
    "\n",
    "The `F1-Score` is combination of the `Precision` and `Recall`. Mathematically this is defined as follows.'\n",
    "\n",
    "\\begin{align}\n",
    "F_1 = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n",
    "\\end{align}\n",
    "\n",
    "We shall use the [**F1_score**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function in sklearn library to calculate the `F1-Score`.\n",
    "\n",
    "It's important to note that the F1-score on the classification task is a way to explicitly measure the performance of the topic models. We use only an out-of-the-box classifier which if further tuned through GridSearch or parameter search should increase performance. However, we wish to use the F1-Score as a way to evaluate the quality of the topic models. We believe that even if we are able to achieve the same performance as using the traditional Bag-of-Words or TFIDF features using the topic models we have a good topic model since we would have indeed reduced the dimensions of the data without affecting the performance.\n",
    "\n",
    "The code for the evaluation metrics is implemented below. We report the `Confusion matrix`, the `Classification report` and the `F1-Score`. However, we only use the weighted `F1-Score` to improve the performance of the Topic Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix'):\n",
    "    # Plot the confusion matrix\n",
    "    sns.set_context('talk')\n",
    "    sns.heatmap(cm, xticklabels=genres, yticklabels=genres, \n",
    "                square=True, annot=True, fmt='.2f')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "\n",
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    # F1_score - weighted by class.\n",
    "    f_score = f1_score(target, predictions, labels=genres, \n",
    "                       pos_label=None, average='weighted')\n",
    "    print('f1_Score : {}'.format(f_score))\n",
    "    \n",
    "    # Confusion matrix of precision and recall\n",
    "    cm = confusion_matrix(target, predictions)\n",
    "    print('confusion matrix\\n {}'.format(cm))\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    # Classification report precision, recall, f_score and support.\n",
    "    print('classification report')\n",
    "    print(classification_report(predictions, target, target_names=genres))\n",
    "    \n",
    "    # Normalizing the confusion matrix because proportions are better than numbers. \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "The raw dataset comprises around 1.5 Million documents scraped from [**fanfiction.net**](https://www.fanfiction.net/) spread across 14 genres. The average length of documents is about 10,000 characters. An initial analysis of a sample of documents revealed that there were languages other than English in the dataset. These were filtered out using a naive stopwords-based language filter and reduced to about 1.38 Million documents primarily in English. Please refer to `lib/Wordvectors/language_filter.py` for how this is accomplished.\n",
    "\n",
    "Within the scope of this project we intend to demonstrate the use of topic models in categorizing a randomly chosen sample of 100,000 documents spread across 5 genres viz. `Humor`, `Sci-fi`, `Family`, `Romance` and `Supernatural`. This decision was made due to scalability constraints. We believe however, that a similar model can be scaled using distributed computing for the original dataset. Kindly, refer to `var/sample_genres.py` and `var/create_csv.py`\n",
    "\n",
    "The dataset is also not free from errors and noise in the form of *spelling errors* and *grammatical errors* since most documents are not proofread and the authors tend to use custom tokens as fillers.\n",
    "\n",
    "In text related NLP the input text is represented as a set of features comprised of `tokens` i.e. a tangible unique set of character(s). [**Tokenization**](#Tokenization) is the process of chopping up a piece of text into pieces, called `tokens` , perhaps at the same time throwing away certain characters, such as punctuation. These are further discussed in the [**Data Pre-processing**](#preprocessing) step along with further feature extraction and selection methods.\n",
    "\n",
    "The final dataset is present in `data_file.csv`. The text has been tokenized before creating the `csv` and has been stripped off the punctuation. It was further noticed that the file contained text that were less than 2000 Characters in length. These were generally *`Author Notes`* and *`Disclaimers`* and added no relevance to the `genre`. They were hence omitted from the dataset. We finally arrived at around 89,000 texts spread across 5 classes in the following ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploratory Visualization\n",
    "With text data any visualization is bound to be not informative unless we perform some sort of preprocessing. This is because, we will be unable to visualize the text in a meaningful manner unless we `tokenize`, `remove stop words` and normalize the text. Hence, we [preprocess](#preprocessing) the text before visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stopword list from Stone, Denis, Kwantes (2010)\n",
    "STOPWORDS = \"\"\"\n",
    "a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be\n",
    "became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can\n",
    "cannot cant co computer con could couldnt cry de describe\n",
    "detail did didn do does doesn doing don done down due during\n",
    "each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen\n",
    "fify fill find fire first five for former formerly forty found four from front full further get give go\n",
    "had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie\n",
    "if in inc indeed interest into is it its itself keep last latter latterly least less ltd\n",
    "just\n",
    "kg km\n",
    "made make many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely\n",
    "neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off\n",
    "often on once one only onto or other others otherwise our ours ourselves out over own part per\n",
    "perhaps please put rather requite rather really regarding\n",
    "same say see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten\n",
    "than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under\n",
    "until up unless upon us used using\n",
    "various very very via\n",
    "was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you\n",
    "your yours yourself yourselves\n",
    "\"\"\"\n",
    "STOPWORDS = frozenset(word for word in STOPWORDS.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stemming \n",
    "stemming = PorterStemmer()\n",
    "def stemmer(story):\n",
    "    # step1. split the document into tokens using spaces,\n",
    "    # step2. if token length is less than 3 ignore the token,\n",
    "    # step3. if token in stop word list ignore the token,\n",
    "    # step4. normalize the case to the token to lower case,\n",
    "    # step5. stem the token using porter stemmer,\n",
    "    # step6. Join the tokens back to a string.\n",
    "    return ' '.join([stemming.stem(word.lower()) for word in story.split() \n",
    "                     if len(word)>3 and word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(f_name):\n",
    "    if os.path.isfile(f_name+'.pickle'):\n",
    "        # if the pickle file exists load it instead.\n",
    "        df = pd.read_pickle(f_name+'.pickle')\n",
    "    else:\n",
    "        df = pd.DataFrame([line.split(',') for line in open(f_name+'.csv')], columns=['genre', 'story'])\n",
    "        \n",
    "        # filter out texts that are less than 2000 chars in length.\n",
    "        df['charlen'] = [len(story) for story in df['story']]\n",
    "        df = df[df['charlen']>2000]\n",
    "        \n",
    "        # stem the tokens in the text\n",
    "        df['story'] = df['story'].map(stemmer)\n",
    "        \n",
    "        # store the pre-processed texts for persistence.\n",
    "        final_data_pickle = os.path.join(settings.project_root, 'tmp', 'data_file.pickle')\n",
    "        df.to_pickle(final_data_pickle)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loc = os.path.join(project_root, 'tmp', 'data_file')\n",
    "data_set = load_data(data_loc)\n",
    "genres = data_set['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_counts(series):\n",
    "    # tokenize each document in an input series and \n",
    "    # update a dictionary of token frequencies.(keys=tokens, values=frequency)\n",
    "    # return a sorted dictionary based on frequencies.\n",
    "    freqs = defaultdict(int)\n",
    "    for story in series:\n",
    "        for token in story.split():\n",
    "            freqs[token] +=1\n",
    "    return OrderedDict(sorted(freqs.items(), key=itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_data(freqs, title, n=25, m=50):\n",
    "    # plot the frequency of n to m tokens from an ordered frequency dictionary. \n",
    "    freqs = OrderedDict(islice(freqs.items(),n,m))\n",
    "    X = np.arange(len(freqs))\n",
    "    sns.set_context(\"talk\")\n",
    "    plt.figure(figsize=(15,5))\n",
    "    sns.barplot(X, freqs.values())\n",
    "    plt.xticks(X, freqs.keys())\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.setp(labels, rotation=90)\n",
    "    ymax = max(freqs.values()) + 0.1\n",
    "    plt.ylim(0, ymax)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the n-m most frequent tokens in the dataset.\n",
    "all_freqs = get_counts(data_set['story'])\n",
    "plot_data(all_freqs, title='{}-{} Most frequent words in the vocabulary'.format(0,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genre_counts(df, n=25,m=50):\n",
    "    # Group by the genres for each genre\n",
    "    # calculate the frequency of the tokens\n",
    "    # plot the frequencies for n to m tokens.\n",
    "    genre_groups = df.groupby('genre')\n",
    "    for genre in df['genre'].unique():\n",
    "        group_freqs = get_counts(genre_groups['story'].get_group(genre))\n",
    "        plot_data(group_freqs, title='{}-{} Most frequent words in {}'.format(0,15,genre),n=25,m=50)\n",
    "# Plot the n to m most frequent tokens in each genre.\n",
    "genre_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that while many words such as `['wash', 'took', 'voice', 'long']` repeat across multiple genres certain words appear more frequently in a few genres `{'Family': ['father', 'mother'], 'Romance': ['felt', 'smile']}` and seem to be representative of the genre. We believe the topic models must be able to make use of these features to arrive at some coherent topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques<a id=\"Algorithms\"></a>\n",
    "\n",
    "Topic modeling is a form of text mining, a way of identifying patterns in a corpus. We take the corpus and run it through a model which groups words across the corpus into `topics`. More formally a topic model assumes that each document is contains a mixture of `topics`. The `topics` are themselves a mixture of `tokens`. These `topics` are what are referred to as Latent features. In this project we use *Latent Dirichlet Allocation* (LDA).\n",
    "\n",
    "#### Latent Dirichlet Allocation<a id=\"LDA\"></a>\n",
    "\n",
    "The basic idea behind [LDA](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) is that documents exhibit multiple topics and are created from a mixture of topics. Formally, a topic is defined as a distribution over a fixed vocabulary. LDA assumes that documents are generated using a distribution of these topics. Operating on this assumption for each document in the collection, it generates the words in a two-stage process.\n",
    "1. Randomly choose a distribution over topics. For instance, a topic is sampled from a [Dirichlet Distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) over a fixed set of *`K`* topics.\n",
    "2. For each word in the document:<br>\n",
    "    a. Randomly choose a topic from the distribution over topics in step 1.<br>\n",
    "    b. Randomly choose a word from the corresponding distribution over the vocabulary.\n",
    "\n",
    "Assuming the above generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection. One way to do this is using [Gibbs Sampling](http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf) algorithm.\n",
    "The algorithm is used as follows:\n",
    "1. for each document `d` in the collection\n",
    "2. go through each word `w` in `d`\n",
    "3. for each topic `t`, compute two things:\n",
    "\n",
    "\\begin{align}\n",
    "p(topic_{t} \\mid document_{d})\n",
    "\\end{align}\n",
    "\n",
    "*The proportion of words in document `D` that are currently assigned to topic `t`.*\n",
    "\n",
    "\\begin{align}\n",
    "p(word_{w} \\mid topic_{t})\n",
    "\\end{align}\n",
    "\n",
    "*The proportion of assignments to topic `t` over all documents that come from this word `w`.*\n",
    "\n",
    "Reassign *`w`* a new topic, where it chooses the topic *`t`* using *`Bayesian Inference`*:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "p(topic_{t} \\mid document_{d}) \\times p(word_{w} \\mid topic_{t}) \n",
    "\\end{align}\n",
    "\n",
    "According to the generative model, this is essentially the probability that topic *`t`* generated word *`w`*, hence the model resamples the current word’s topic with this probability.Given enough number of iterations using the Gibbs Sampling algorithm this model forms cohesive topics of the given text. A more detailed explaination and introduction to LDA is presented in this [paper](https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf) by David M. Blei, one of the authors of the original papers.\n",
    "\n",
    "There are two parameters of the `dirichlet distribution` that play an important role in **LDA** namely:\n",
    "1. $\\alpha$ (`doc_topic_prior`) controls the per-document topic distribution.\n",
    "   A high $\\alpha$ value means that every documents is likely to contain a mixture of most of the topics and not a specific topic, while a low alpha implies that a document is more likely to be represented by a few topics.\n",
    "\n",
    "2. $\\beta$ (`topic_word_prior`) controls the per-topic word distribution.<br>\n",
    "   A high $\\beta$ value means that every topic is likely to contain a mixture of most of the words, while a low  $\\beta$ value means that a topic may contain a mixture of just a few of the words.\n",
    "\n",
    "Intuitively, high $\\alpha$ makes documents appear more similar to each other, High $\\beta$ makes topics appear more similar to each other. This [video](https://www.youtube.com/watch?v=3mHy4OSyRf0) was a reference to understand the importance of the hyperparameters.\n",
    "\n",
    "We implement the algorithm using the [LatentDirchletAllocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn-decomposition-latentdirichletallocation)implementation in sklearn and intialize with the following parameters:\n",
    "```\n",
    "LatentDirichletAllocation(n_topics=5, doc_topic_prior=None, topic_word_prior=None, learning_method='online', learning_decay=0.7, max_iter=250, batch_size=1000, n_jobs=-1, random_state=42)\n",
    "```\n",
    "\n",
    "#### Classifier\n",
    "For the classification task use a Stochastic Gradient Descent Classifier(`SGDClassifier`). We use the classifier as a way to explicitly measure the performance of the topic models generated by `LDA`. `SGDClassifier` is a linear model and is a widely used algorithm in text classification. They operate by separating the samples into classes in a hyperplane. They are computationally efficient and scale to a large number of samples.\n",
    "\n",
    "We use the [SGDClassfier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn-linear-model-sgdclassifier) algorithm for the classification tasks with the following initial parameters:\n",
    "```\n",
    "sklearn.linear_model.SGDClassifier('penalty'='elasticnet', 'loss'='log', 'n_iter'= 25, 'shuffle'= True', 'class_weight'='balanced', 'n_jobs': -1, 'random_state'=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "We benchmark the classification task using a classifier that performs random guesses. This is generally used as a baseline only if all the classes are uniformly distributed. Since, this is true for this dataset ie. we have 5 uniformly distributed classes, we proceed with a `dummy_classifier` that performs random guesses. We believe that the introduction of features using Feature extraction methods such as `Bag-Of-Words` and `TF-IDF` will increases the performance of the classifier. With the introduction of the `topic model` the classifier must be able to do better than a random guess and at least as good as the `Bag-of-words` and `TF-IDF` if not better for us to be able to claim that the topic model is useful in reducing the dimensions of the dataset whilst forming `topics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_predictor(samples):\n",
    "    # for each sample in the input predict a random genre.\n",
    "    return np.random.choice(genres, len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_predictions = dummy_predictor(test_data['story'])\n",
    "evaluate_prediction(baseline_predictions, test_data['genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing<a name=\"preprocessing\"></a>\n",
    "\n",
    "As discussed in the data exploration section earlier, in NLP tasks [pre-processing](#preprocessing) of the text is quite an important task. A machine learning algorithm cannot work with text as input. We must therefore transform the input collection of documents to a meaningful mathematical form representative of the text. Before we begin discussing the transformations it is important to understand that these steps are similar to feature engineering in traditional machine learning and is often an iterative process.\n",
    "\n",
    "The pre-processing module in this project applies a series of transformations on the text to arrive at a mathematical representation of the text. These are discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization<a name=\"Tokenization\"></a>\n",
    "\n",
    "This is a process of representing a document as a list of `tokens`. The token can be based on pre-defined characteristics such as words(unigrams), two-word pairs(bigram), three-word-sets(trigram) and so on. `Tokens` may also defined to be a group of n-characters (ex: character trigram). In this project we choose to use unigrams since it is one of the most commonly used representations. Thus, a document can be represented as a list of unigram tokens. For instance, we can tokenize the following sentence `\"Machine Learning is fun.\"` as a list of unigram tokens to `['Machine', 'Learning', 'is', 'fun', '.']`. While this is the general idea, tokenization of real text is often complex due to irregularities in the text.\n",
    "\n",
    "We use the NLTK package’s regular expression tokenizer [**`nltk.tokenize.RegexpTokenizer`**](http://www.nltk.org/_modules/nltk/tokenize/regexp.html) to tokenize words. We further normalize the case of the text and turn all text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example tokenization on a sample document.\n",
    "sample_texts = [\"He has a very big dog.\",\"He owns a bike.\",\n",
    "                \"The dog knows how to ride a bike\", \"He wanted to walk his dog.\",\n",
    "                \"His dog was at home.\", \"He went home on his bike.\",\n",
    "                \"The dog barked as he reached.\", \"His cycled with his dog.\"]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokened_texts = [tokenizer.tokenize(sample.lower()) \n",
    "                 for sample in sample_texts]\n",
    "print(tokened_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal<a name=\"Stopwords\"></a>\n",
    "\n",
    "In NLP text processing **stopwords** refer to words that are removed from a corpus of documents when an index is created from the text data. Generally, these are the most commonly used functional words such as `the`, `is`, `a` etc. Some NLP tools exclusively avoid removal of stopwords for reasons such as preserving context. We will be removing them since they generally affect the performance of topic models such as [`Latent Dirichlet Allocation`](#LDA). \n",
    "\n",
    "Besides these funtional stop words the implementation in this project uses the parameters `min_df`, `max_df` and `max_features` in the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) class to remove additional stop words that appear less than 10 times and in more than 75% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_tokens = [[word for word in tokened_text if not word in STOPWORDS]\n",
    "                   for tokened_text in tokened_texts]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming<a name=\"stem\"></a>\n",
    "\n",
    "Stemming is a process of reducing a word to a common root, often ignoring the syntactic and semantic context of the token. For example, the words *`being`*, *`am`*, *`are`*, *`is`*  are reduced to *`be`*. Text documents often for grammatical reasons, use different forms of the same word in different contexts. While this might be helpful to derive grammatical meaning, since these words do not differ in their semantic usage we could stem the token and thus benefit from the reduced size of the vocabulary. We perform stemming using the [`PorterStemmer`](http://www.nltk.org/api/nltk.stem.html#nltk.stem.porter.PorterStemmer) algorithm in the [nltk](http://www.nltk.org/#natural-language-toolkit) library. It's a very popular stemmer in natural language processing and is used in a variety of classification tasks as a pre-processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Eample of Stemming the text with the PorterStemmer.\n",
    "demo_stemmer = PorterStemmer()\n",
    "token_stems = [[demo_stemmer.stem(token) for token in filtered_token] \n",
    "                for filtered_token in filtered_tokens] \n",
    "print(token_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-Of-Words<a name=\"BoW\"></a>\n",
    "\n",
    "A `bag-of-words(BoW)` is a matrix representation of the text corpus. The terms/tokens in the vocabulary form the rows while the documents in the corpus form the columns of the matrix. Since, not all tokens in the vocabulary are present in all documents the `BoW` model is often a Sparse Matrix. The values in the matrix is the frequency of occurrence of the token in the documents. This is constructed using the [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) class in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example BoW model matrix for a series of documents.\n",
    "\n",
    "pre_processed_text = [' '.join(text) for text in token_stems]\n",
    "demo_bow_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                      tokenizer=word_tokenize)\n",
    "\n",
    "demo_bow_model = demo_bow_vectorizer.fit_transform(pre_processed_text)\n",
    "col_names = ['text'+str(i+1) for i in range(len(pre_processed_text))]\n",
    "demo_df = pd.DataFrame(demo_bow_model.todense().T,\n",
    "                  index=demo_bow_vectorizer.get_feature_names(),\n",
    "                  columns=col_names)\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Matrix<a name=\"TF-IDF\"></a>\n",
    "\n",
    "The `Term frequency-Inverse document frequency`(TFIDF) matrix is a representation of the [BoW](#BoW) model.It augments the representaiton by scoring the importance of `terms`(words) in a document based on how frequently they appear in multiple documents. Intuitively, if a word appears multiple times in a document, it gets a huge score. However, if it also appears in multiple documents it's not a unique word hence, it is given a low score. We use [`Tfidftransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn-feature-extraction-text-tfidftransformer) to transform the BoW model to a TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example of Tfidf Transformation of a BoW matrix.\n",
    "demo_tfidf_vectorizer = TfidfTransformer()\n",
    "demo_tfidf_model = demo_tfidf_vectorizer.fit_transform(demo_bow_model)\n",
    "\n",
    "col_names = ['text'+str(i+1) for i in range(len(pre_processed_text))]\n",
    "demo_df = pd.DataFrame(demo_tfidf_model.todense().T, \n",
    "                       index=demo_bow_vectorizer.get_feature_names(),\n",
    "                       columns=col_names)\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementation\n",
    "\n",
    "#### Read and preprocess\n",
    "\n",
    "We begin with importing the pre-processed CSV file into a pandas Dataframe. We first process the text, removing `stop-words` and `stemming` the text while also case normalizing the text and writing back the normalized text to the dataframe. Further, we shuffle the data in this step. We also pickle the data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = load_data(data_loc)\n",
    "data_set = data_set.sample(frac=1).reset_index(drop=True)\n",
    "genres = data_set['genre'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split.\n",
    "\n",
    "Next we split the Dataframe into a train and test sets using the sklearn [train-test split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn-cross-validation-train-test-split) function. The test set is generally kept hidden from both the feature transformers and the classifier. This is to ensure that the no data leaks from the test to the train set during the feature transformation and extraction stages. We only use the test set to evaluate the final performance in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# at 89,000 instance even 8900 instace forms a good test set.\n",
    "train_data, test_data = train_test_split(data_set, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Features Model Performance\n",
    "\n",
    "Initially, we implement the [Bag-Of-Words](#BoW) model using the `CountVectorizer` class as demonstrated earlier. We fit the vectorizer on the training set and transform the training and the test sets. Note that we do not train on the test set and do not include it in the vocabulary. To get an idea of the quality of features we measure the performance of the `SGDClassifier` using the [evaluation metrics](#metrics) defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the countvectorizer with the intial parameters.\n",
    "bow_params ={'analyser': 'word',\n",
    "             'tokenizer': word_tokenize,\n",
    "             'max_df': 0.5,\n",
    "             'max_features': 75000,\n",
    "             'min_df': 10\n",
    "             } \n",
    "bow_vectorizer = CountVectorizer(**bow_params)\n",
    "\n",
    "# train on and transform the train-set\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_data['story'])\n",
    "\n",
    "# transform the test-set\n",
    "bow_test_features = bow_vectorizer.transform(test_data['story'])\n",
    "\n",
    "# Initialize the classifier with intial parameters.\n",
    "clf_params = {'class_weight': 'balanced', \n",
    "              'loss': 'log', \n",
    "              'n_iter': 25, \n",
    "              'penalty': 'elasticnet', \n",
    "              'random_state': 42, \n",
    "              'shuffle': True,\n",
    "              'n_jobs': -1,\n",
    "              'warm_start': False\n",
    "              }\n",
    "classifier = SGDClassifier(**clf_params)\n",
    "\n",
    "# train the classifer using the BOW features\n",
    "classifier.fit(bow_train_features, train_data['genre'])\n",
    "\n",
    "# predict the classes on the test set.\n",
    "bow_predictions = classifier.predict(bow_test_features)\n",
    "\n",
    "# evaluate performance on the transformed test set.\n",
    "evaluate_prediction(bow_predictions, test_data['genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Features Model Performance\n",
    "\n",
    "We transform the [`Bag-of-Words`](#BoW)(BoW) matrix using a [TFIDFTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn-feature-extraction-text-tfidftransformer) to a [`TF-IDF matrix`](#TF-IDF). Here, we make use of the sklearn [**Pipeline**](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn-pipeline-pipeline) class to first transform the features from the BoW matrix to a TF-IDF-Matrix and then train the `LinearSVC`. The advantage of using such a pipeline is that we can perform the same chained transformations on the test set during prediction. We also measure the performance of the classifier using the TF-IDF features on the test set and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize pipeline with tf-idf transformation.\n",
    "tfidf_params = {'tfidf__norm': 'l1',\n",
    "                'tfidf__use_idf': True\n",
    "                }\n",
    "tfidf_pipeline = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                           ('clf', classifier)\n",
    "                           ])\n",
    "tfidf_pipeline.set_params(**tfidf_params)\n",
    "\n",
    "# train the pipeline using th Bag-of-Words features.\n",
    "tfidf_pipeline.fit(bow_train_features, train_data['genre'])\n",
    "\n",
    "# predict on the test Bag-of-words features.\n",
    "tfidf_predictions = tfidf_pipeline.predict(bow_test_features, test_data['genre'])\n",
    "\n",
    "# evaluate performance on transformed test set.\n",
    "evaluate_prediction(tfidf_predictions, test_data['genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Features Model Performance\n",
    "\n",
    "The LDA algorithm is implemented on both the BoW and the TF-IDF independently. The performance is reported individually on both feature models. Although the performance has not improved, there are hyperparameters that need to be tuned to improve the performance of the LDA model. These include the `n_topics`, `doc_topic_prior` and `topic_word_prior`. The tuning of these hyperparameters is further discussed in the [Refinement](#refine) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize pipeline with lda transformation.\n",
    "\n",
    "bow_lda_pipeline = Pipeline([('lda', LatentDirichletAllocation()),\n",
    "                             ('clf', classifier)\n",
    "                             ])\n",
    "\n",
    "# train the pipeline using the Bag-of-Words features\n",
    "bow_lda_pipeline.fit(bow_train_features, train_data['genre'])\n",
    "\n",
    "# predict on the test Bag-of-words features.\n",
    "bow_lda_predictions = bow_lda_pipeline.predict(bow_test_features)\n",
    "\n",
    "# evaluate performance on transformed test set.\n",
    "evaluate_prediction(bow_lda_predictions, test_data['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize pipeline with tf-idf and lda transformations.\n",
    "tfidf_lda_params = tfidf_param.copy()\n",
    "tfidf_lda_params.update(lda_params)\n",
    "\n",
    "tfidf_lda_pipeline = Pipeline([('tfidf': TfidfTransformer()),\n",
    "                               ('lda', LatentDirichletAllocation()),\n",
    "                               ('clf', classifier)\n",
    "                              ])\n",
    "tfidf_lda_pipeline.set_params(**tfidf_lda_params)\n",
    "\n",
    "# train the pipeline using th Bag-of-Words features\n",
    "tfidf_lda_pipeline.fit(bow_train_features, train_data['genre'])\n",
    "\n",
    "# predict on the test Bag-of-words features.\n",
    "tfidf_lda_predictions = tfidf_lda_pipeline.predict(bow_test_features)\n",
    "\n",
    "# evaluate performance on transformed test set.\n",
    "evaluate_prediction(tfidf_lda_predictions, test_data['genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "Here we discuss the refinement process we follow to improve our model. We use [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn-grid-search-randomizedsearchcv) for finding the hyperparameters of the pipelines described above.\n",
    "\n",
    "Specifically, we first construct a pipeline for the `bow_lda model` and the `tfidf_lda_model`, run `RandomizedSearchCV` and report the `grid_scores` and `best_params`. Next, we predict using the new set of parameters on the test set and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize pipelines\n",
    "ref_bow_lda_pipeline = Pipeline([('lda', LatentDirichletAllocation()),\n",
    "                                 ('clf', classifier)\n",
    "                                 ])\n",
    "\n",
    "ref_tfidf_lda_pipeline = Pipeline([('lda', LatentDirichletAllocation()),\n",
    "                                   ('clf', classifier)\n",
    "                                  ])\n",
    "\n",
    "# initalize parameters for RandomizedSearchCV\n",
    "\n",
    "lda_params = {'lda__n_topics': [10, 30, 150, 250],\n",
    "              'lda__max_iter': [30, 100, 250]\n",
    "              'lda__doc_topic_prior': [None, 0.3, 0.9],\n",
    "              'lda__topic_word_prior': [None, 0.3, 0.9],\n",
    "              'lda__learning_decay': [0.6, 0.9]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize and search \n",
    "bow_lda_search = RandomizedSearchCV(estimator=ref_bow_lda_pipeline,\n",
    "                                    param_distributions=lda_params,\n",
    "                                    n_iter=15,\n",
    "                                    scoring='f1_weighted',\n",
    "                                    random_state=42,\n",
    "                                    n_jobs=-1)\n",
    "bow_lda_search.fit(bow_train_features, train_data['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('grid scores for the Randomized search bow-lda')\n",
    "print(bow_lda_search.grid_scores)\n",
    "print('best parameters: {}'.format(bow_lda_search.best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict using best parameters.\n",
    "bow_lda_seach_predictions = bow_lda_search.predict(bow_test_features)\n",
    "evaluate_prediction(bow_lda_search_predictions, test_data['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we initialize Tfidftransformer separately.\n",
    "# this reduces the transformation overhead while GridSearch \n",
    "tfidf_vectorizer = TfidfTransformer(**tfidf_params)\n",
    "\n",
    "# train the transformer using the Bag-of-Words train_features\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(bow_train_features)\n",
    "\n",
    "# transform the test-set\n",
    "tfidf_test_features = tfidf_vectorizer.transform(bow_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "tfidf_lda_search = RandomizedSearchCV(estimator=ref_tfidf_lda_pipeline,\n",
    "                                      param_distributions=lda_params,\n",
    "                                      n_iter=15,\n",
    "                                      scoring='f1_weighted',\n",
    "                                      random_state=42,\n",
    "                                      n_jobs=-1)\n",
    "# fit and search the hyperparameter space\n",
    "tfidf_lda_search.fit(tfidf_train_features, train_data['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('grid scores for the Randomized search of Tfidf-lda')\n",
    "print(tfidf_lda_search.grid_scores)\n",
    "print('best parameters: {}'.format(tfidf_lda_search.best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict using best parameters.\n",
    "tfidf_lda_seach_predictions = tfidf_lda_search.predict(tfidf_test_features)\n",
    "evaluate_prediction(tfidf_lda_search_predictions, test_data['genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Model Evaluation, Validation and Visualization\n",
    "\n",
    "Topic models are often evaluated by human experts by verifying the coherence of topics. This is often measured by how similar words/tokens categorized under a topic are. Although we explicitly measured the performance of the topics using the `f1-weighted` score, we shall further investigate the resulting topics using visualizations. We employ the [pyLDAVis](https://pyldavis.readthedocs.io/en/latest/readme.html#pyldavis) library to perform the topic visualizations below. In this visualization we inspect the best topic model alone on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda_topic_model, vectorized_test_set, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at the above topics it's quite clear that topic 1 is about ...  topic 2 is about ...  topic 3 is about ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Reflections and Discussion\n",
    "\n",
    "Natural Language processing is quite a challenging endeavour and true to its nature creating good models for text data is a daunting task. This arises from the various characteristics of text data such as the number of features the presence of outliers, etc. We began with the objective of creating a coherent topic model of the fan-fiction dataset using `probabilistic topic modelling`. Concretely we had planned to use LDA to find coherent topics that can then be used to describe the collection of documents as a group of topics. To this end we pre-processed the documents, extracted features in the form of document-term matrices and applied LDA on the features. We evaluated the performance of the LDA model using a classifier. The results.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "\n",
    "Our task at hand was to improve the "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
