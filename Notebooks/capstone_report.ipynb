{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "## Machine Learning Engineer Nanodegree\n",
    "\n",
    "Bharat Ramanathan<br>\n",
    "September 20, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "Today, computers perform a myriad of tasks and yet we are unable to communicate with them naturally. We humans use words, sound and gestures to communicate our ideas, thoughts and feelings. Yet computers only understand explicit instructions, that require complex artificial programming languages. Natural Language Processing (NLP) is a field of Machine Learning and AI that is concerned with human computer interaction using Natural Language.  NLP aims to break the language barrier and allows us to interact with computers in a natural manner. Advances in NLP research is one of the primary reasons for the development of personal assistants, voice commands and search etc.\n",
    "\n",
    "In this project we are concerned with the ability of systems to process and understand written text. Natural Language processing is often difficult due to the lack of a strict structure in human languages and a variety of anomalies present in them. All this messiness result in challenges to process text in a meaningful way. A rule based system is not viable since the rules of language are plenty, often unrestricted and change over time.\n",
    "\n",
    "With text documents it often becomes difficult to find and discover what we are looking for. Traditionally this is done using search keywords and links. Imagine  instead the ability to search and explore documents based on the themes that run through them. We might first find the theme that we are interested in, and then examine the documents related to that theme. This project is about discovering thematic similarities in a corpus of documents using Machine Learning and Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Searching fan-fiction documents is an onerous task. This is further emphasized when search tags are quite ineffective in finding relevant documents and seldom consider context and themes while retrieving documents. While document search is augmented by SEO keywords they can be misleading often on purpose to improve page visits and user clicks. Other tags include genre tags which although quite accurate prove to be too general for the task of search. Users often struggle with finding the necessary documents using only genres.\n",
    "\n",
    "We seek to address the problem of document search by forming meaningful thematic categories that can be used to find relevant documents. Furthermore, these thematically similar documents can be utilized to build recommender systems. We intend to explore *probabilistic topic models* that represent documents as being generated from a group of topics. One way to think about the process of topic modelling is to assume that each document in a collection contains a mixture of topics. A topic can be thought of as a distribution of words that appear in similar syntactic and semantic context. Of course, all we have to begin with are the documents but the model specifies a simple probabilistic procedure by which documents can be generated from a distribution of topics. Standard statistical techniques are then used to invert this process, inferring the set of topics that were responsible for generating a collection of documents. We will further discuss topic models and how they work in [Algorithms and Techniques](#Algorithms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "The model performance can be evaluated by testing its ability to categorize documents into genres and verifying the category labels against a pre-existing set of genre labels i.e. the ground truth labels. We achieve this by clustering the documents into topics while choosing the number of clusters to be equal to the number of genres. The genre of the document at the cluster center or centroid is assumed to be the genre of the documents in the cluster. The cluster quality can then be evaluated by finding the `F1-Score` calculated between cluster or genre a document is predicted to be in and the ground truth label i.e. the genre-tag of the document.\n",
    "\n",
    "The `F1-Score` is combination of the `Precision` and `Recall`. Mathematically this is as follows.'\n",
    "\n",
    "\\begin{align}\n",
    "F_1 = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n",
    "\\end{align}\n",
    "\n",
    "We shall use the [**F1_score**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function in sklearn library to calculate the `F1-Score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw dataset copmrises of around 1.5 Million documents scraped from [**fanfiction.net**](https://www.fanfiction.net/) spread across 14 genres. The average length of documents is about 10,000 characters. An initial analysis of a sample of documents revealed that there were languages other than English in the dataset. These were filtered out using a naive stopwords based language filter and reduced to about 1.38 Million documents primarily in English.\n",
    "\n",
    "\n",
    "Within the scope of this project we intend to demonstrate the use of topic models in categorizing a randomly chosen sample of 100,000 documents spread across 5 genres viz. `Humor`, `Sci-fi`, `Family`, `Romance` and `Supernatural`. This decision was made due to scalability constraints. We believe however, that a similar model can be scaled using distributed computing for the original dataset.\n",
    "\n",
    "The dataset is also not free from errors and noise in the form of *spelling errors* and *grammatical errors* since most documents are not proofread and the authors tend to use custom tokens as fillers.\n",
    "\n",
    "In text related NLP the input text is represented as a set of features comprised of `tokens` i.e. a tangible unique set of character(s). [**Tokenization**](#Tokenization) is the process of chopping up a piece of text into pieces, called `tokens` , perhaps at the same time throwing away certain characters, such as punctuation. These are further discussed in the [**Data Pre-processing**](#preprocessing) step along with further feature extraction and selection methods. The below table of common tokens found in the corpus ordered by frequency of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploratory Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques<a id=\"Algorithms\"></a>\n",
    "\n",
    "Topic modeling is a form of text mining, a way of identifying patterns in a corpus. You take your corpus and run it through a tool which groups words across the corpus into ‘topics’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing<a name=\"preprocessing\"></a>\n",
    "\n",
    "As discussed in the data exploration section earlier, in NLP tasks pre-processing of the text is quite an important task. A machine learning algorithm cannot work with text as input. We must therefore transform the input collection of documents to a meaningful  mathematical form representative of the text. Before we begin discussing the transformations it is important to understand that these steps are similar to feature engineering in traditional machine learning and is often an iterative process.\n",
    "\n",
    "The pre-processing module in this project applies a series of transformations on the text to arrive at a mathematical representation of the text. These are discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization<a name=\"Tokenization\"></a>\n",
    "\n",
    "The is a process of representing a document as a list of `tokens`. The token can be based on pre-defined characteristics such as words(unigrams), two-word pairs(bigram), three-word-sets(trigram) and so on. `Tokens` may also defined to be a group of n-characters (ex: character trigram). In this project we choose to use unigrams since it is one of the most commonly used representations. Thus, a document can be represented as a list of unigram tokens. For instance, we can tokenize the following sentence `\"Machine Learning is fun.\"` as a list of unigram tokens to `['Machine', 'Learning', 'is', 'fun']`. While this is the general idea, tokenization of real text is often complex due to irregularities in the text.\n",
    "\n",
    "We use the NLTK package’s regular expression tokenizer [**`nltk.tokenize.RegexpTokenizer`**](http://www.nltk.org/_modules/nltk/tokenize/regexp.html) to tokenize words. We further normalize the case of the text and turn all text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'has', 'a', 'very', 'big', 'dog'], ['he', 'knows', 'how', 'to', 'ride', 'a', 'bike'], ['he', 'wanted', 'to', 'find', 'a', 'job'], ['he', 'wanted', 'to', 'talk', 'to', 'his', 'boss'], ['he', 'went', 'to', 'the', 'post', 'office'], ['his', 'wife', 'is', 'at', 'work', 'right', 'now'], ['his', 'wife', 'worked', 'in', 'the', 'house']]\n"
     ]
    }
   ],
   "source": [
    "# Example Tokenization on a sample document.\n",
    "sample_texts = [\"He has a very big dog.\",\"He knows how to ride a bike.\",\n",
    "                \"He wanted to find a job.\",\"He wanted to talk to his boss.\",\n",
    "                \"He went to the post office.\",\"His wife is at work right now.\",\n",
    "                \"His wife worked in the house.\"]\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokened_texts = [tokenizer.tokenize(sample.lower()) for sample in sample_texts]\n",
    "print(tokened_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal<a name=\"Stopwords\"></a>\n",
    "\n",
    "In NLP text processing **stopwords** refer to words that are removed from a corpus of documents when an index is created from the text data. Generally, these are the most commonly used functional words such as `the`, `is`, `a` etc. Some NLP tools exclusively avoid removal of stopwords for reasons such as preserving context. We will be removing them since they generally affect the performance of semantic models such as `Latent Dirichlet Allocation`. \n",
    "\n",
    "The original implementation in the project uses the [**`filter_tokens`**](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_tokens) method to accomplish this. We further use the [**`filter_extremes`**](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes) method to remove words that appear less than 5 times and in more than 75% of the documents. This can be accomplished using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['big', 'dog'], ['knows', 'ride', 'bike'], ['wanted', 'job'], ['wanted', 'talk', 'boss'], ['went', 'post', 'office'], ['wife', 'work', 'right'], ['wife', 'worked', 'house']]\n"
     ]
    }
   ],
   "source": [
    "# Example of stopword removal\n",
    "STOPWORDS = \"\"\"\n",
    "a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be\n",
    "became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can\n",
    "cannot cant co computer con could couldnt cry de describe\n",
    "detail did didn do does doesn doing don done down due during\n",
    "each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen\n",
    "fify fill find fire first five for former formerly forty found four from front full further get give go\n",
    "had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie\n",
    "if in inc indeed interest into is it its itself keep last latter latterly least less ltd\n",
    "just\n",
    "kg km\n",
    "made make many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely\n",
    "neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off\n",
    "often on once one only onto or other others otherwise our ours ourselves out over own part per\n",
    "perhaps please put rather re\n",
    "quite\n",
    "rather really regarding\n",
    "same say see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten\n",
    "than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under\n",
    "until up unless upon us used using\n",
    "various very very via\n",
    "was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you\n",
    "your yours yourself yourselves\n",
    "\"\"\"\n",
    "STOPWORDS = frozenset(word for word in STOPWORDS.split())\n",
    "filtered_tokens = [[word for word in tokened_text if not word in STOPWORDS] for tokened_text in tokened_texts]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming<a name=\"Stemming\"></a>\n",
    "\n",
    "Stemming is a process of reducing a word to a common root based on some crude heuristics. For example, the words `being`, `am`, `are`, `is`  is reduced to `be`. Text documents often for grammatical reasons, use different forms of the same word in different contexts. While this might be helpful to derive grammatical meaning, since these words do not differ in their semantic usage we could stem the words and thus benefit from the reduced size of the vocabulary. Lemmatization is an alternate process that produces `lemmas` which preserves the roots’ syntactic and semantic usage. However, it’s an expensive process. Therefore we perform stemming using the [`PorterStemer`](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/parsing/porter.py) algorithm in the [`gensim`](https://radimrehurek.com/gensim/) library. [`gensim.parsing.preprocesssing`](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/parsing/preprocessing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['big', 'dog'], ['know', 'ride', 'bike'], ['want', 'job'], ['want', 'talk', 'boss'], ['went', 'post', 'offic'], ['wife', 'work', 'right'], ['wife', 'work', 'hous']]\n"
     ]
    }
   ],
   "source": [
    "# Eample of Stemming the text with the gensim stemmer.\n",
    "from gensim.parsing import porter\n",
    "stemmer = porter.PorterStemmer()\n",
    "stemmed_tokens = [[stemmer.stem(token) for token in filtered_token] for filtered_token in filtered_tokens] \n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary\n",
    "\n",
    "A `dictionary` is a mapping of all the tokens in the vocabulary of a corpus to unique numbers. After [tokenization](#Tokenization), [stopword removal](#Stopwords) and [stemming](#Stemming), creating the `dictionary` is one of the first steps in the transformation of the corpus from the initial text to a mathematical form. This is achieved by using an instance of [`gensim.corpora.Dictionary`](https://radimrehurek.com/gensim/corpora/dictionary.html) class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'right': 13, 'wife': 14, 'job': 5, 'big': 0, 'ride': 2, 'hous': 15, 'dog': 1, 'boss': 8, 'offic': 11, 'bike': 3, 'know': 4, 'want': 6, 'post': 9, 'work': 12, 'went': 10, 'talk': 7}\n"
     ]
    }
   ],
   "source": [
    "# Example of the dictionary creation with gensim\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(stemmed_tokens)\n",
    "print_dict = {}\n",
    "for item, value in dictionary.iteritems(): \n",
    "    print_dict[str(value)] = item\n",
    "print(print_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-Of-Words<a name=\"BoW\"></a>\n",
    "\n",
    "A `bag-of-words(BoW)` is a matrix representation of the text corpus. The terms/tokens in the vocabulary form the rows while the documents in the corpus form the columns of the matrix. Since, not all words in the vocabulary are present in all documents the `BoW` model is often a Sparse Matrix. The values in the matrix is the frequecny of occurance of the word in the documents. This is constructed using the [`gensim.corpora.Dictionary.doc2bow`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow) function in the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_3</th>\n",
       "      <th>text_4</th>\n",
       "      <th>text_5</th>\n",
       "      <th>text_6</th>\n",
       "      <th>text_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bike</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boss</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hous</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ride</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wife</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_1  text_2  text_3  text_4  text_5  text_6  text_7\n",
       "big       1.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "bike      1.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
       "boss      0.0     1.0     0.0     0.0     0.0     0.0     0.0\n",
       "dog       0.0     1.0     0.0     0.0     0.0     0.0     0.0\n",
       "hous      0.0     1.0     0.0     0.0     0.0     0.0     0.0\n",
       "job       0.0     0.0     1.0     0.0     0.0     0.0     0.0\n",
       "know      0.0     0.0     1.0     1.0     0.0     0.0     0.0\n",
       "offic     0.0     0.0     0.0     1.0     0.0     0.0     0.0\n",
       "post      0.0     0.0     0.0     1.0     0.0     0.0     0.0\n",
       "ride      0.0     0.0     0.0     0.0     1.0     0.0     0.0\n",
       "right     0.0     0.0     0.0     0.0     1.0     0.0     0.0\n",
       "talk      0.0     0.0     0.0     0.0     1.0     0.0     0.0\n",
       "want      0.0     0.0     0.0     0.0     0.0     1.0     1.0\n",
       "went      0.0     0.0     0.0     0.0     0.0     1.0     0.0\n",
       "wife      0.0     0.0     0.0     0.0     0.0     1.0     1.0\n",
       "work      0.0     0.0     0.0     0.0     0.0     0.0     1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Example of BoW model matrix for a series of documents.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in stemmed_tokens]\n",
    "cols = ['text_{}'.format(i+1) for i in range(len(sample_texts))]\n",
    "df = pd.DataFrame(gensim.matutils.corpus2dense(bow_corpus, num_terms=len(dictionary)), columns=cols)\n",
    "df = df.set_index(pd.Series(print_dict).index)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Matrix\n",
    "\n",
    "The `Term frequency-Inverse document frequency`(TFIDF) matrix is a representation of the [BoW](#BoW) mode. However, it scores the of importance of `terms`(words) in a document based on how frequently they appear in multiple documents. Intuitively, if a word appears multiple times in a document, it gets a hige score. However, if it also appears in multiple document it's not a unique word hence, give it a low score. We use [`gensim.models.tfidfmodel.TfidfModel`](https://radimrehurek.com/gensim/models/tfidfmodel.html) to transform the BoW model to a TF-IDF matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_3</th>\n",
       "      <th>text_4</th>\n",
       "      <th>text_5</th>\n",
       "      <th>text_6</th>\n",
       "      <th>text_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bike</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boss</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hous</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.840820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.541314</td>\n",
       "      <td>0.414319</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offic</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643560</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643560</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ride</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.476043</td>\n",
       "      <td>0.476043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.739436</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wife</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.476043</td>\n",
       "      <td>0.476043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.739436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_1   text_2    text_3    text_4   text_5    text_6    text_7\n",
       "big    0.707107  0.00000  0.000000  0.000000  0.00000  0.000000  0.000000\n",
       "bike   0.707107  0.00000  0.000000  0.000000  0.00000  0.000000  0.000000\n",
       "boss   0.000000  0.57735  0.000000  0.000000  0.00000  0.000000  0.000000\n",
       "dog    0.000000  0.57735  0.000000  0.000000  0.00000  0.000000  0.000000\n",
       "hous   0.000000  0.57735  0.000000  0.000000  0.00000  0.000000  0.000000\n",
       "job    0.000000  0.00000  0.840820  0.000000  0.00000  0.000000  0.000000\n",
       "know   0.000000  0.00000  0.541314  0.414319  0.00000  0.000000  0.000000\n",
       "offic  0.000000  0.00000  0.000000  0.643560  0.00000  0.000000  0.000000\n",
       "post   0.000000  0.00000  0.000000  0.643560  0.00000  0.000000  0.000000\n",
       "ride   0.000000  0.00000  0.000000  0.000000  0.57735  0.000000  0.000000\n",
       "right  0.000000  0.00000  0.000000  0.000000  0.57735  0.000000  0.000000\n",
       "talk   0.000000  0.00000  0.000000  0.000000  0.57735  0.000000  0.000000\n",
       "want   0.000000  0.00000  0.000000  0.000000  0.00000  0.476043  0.476043\n",
       "went   0.000000  0.00000  0.000000  0.000000  0.00000  0.739436  0.000000\n",
       "wife   0.000000  0.00000  0.000000  0.000000  0.00000  0.476043  0.476043\n",
       "work   0.000000  0.00000  0.000000  0.000000  0.00000  0.000000  0.739436"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf_model = TfidfModel(bow_corpus, dictionary=dictionary)\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "tdf = pd.DataFrame(gensim.matutils.corpus2dense(tfidf_corpus, num_terms=len(dictionary)), columns=cols)\n",
    "tdf = tdf.set_index(pd.Series(print_dict).index)\n",
    "display(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
